파이썬과 케라스로 배우는 강화학습
=============

일자 별 학습 내용
-------------
- 2024.09.04
    - 1장 강화학습 개요   
    강화학습 개념 - 행동심리학과 강화학습, 머신러닝과 강화학습 간의 연관성, '강화'라는 개념   
    강화학습 문제 - 강화학습은 순차적 행동 결정 문제에 적용   
    강화학습의 예시:브레이크아웃 - 브레이크아웃의 MDP와 학습방법   
    - 2장 강화학습 기초1:MDP와 벨만방정식   
    MDP - 상태, 행동, 보상함수, 상태변환확률, 할인율, 정책   
    가치함수 - 반환값 G, 가치함수 v(s), 큐함수 q(s,a)   
- 2024.09.05   
    - 2장 강화학습 기초1:MDP와 벨만방정식   
    벨만 방정식 - 현재 상태의 가치함수와 다음 상태 가치함수의 관계식을 정의한 것.   
                 벨만 기대 방정식, 벨만 최적 방정식   
    - 3장 강화학습 기초2:그리드월드와 다이내믹 프로그래밍   
    다이내믹 프로그래밍과 그리드 월드 - 쿤 문제를 바로 푸는 것이 아닌 작은 문제들을 풀어나감   
                                     정책 이터레이션, 가치 이터레이션   
    다이내믹 프로그래밍1: 정책 이터레이션 - 정책 평가 -> 정책 발전   
- 2024.09.06   
    - 3장 강화학습 기초2:그리드월드와 다이내믹 프로그래밍   
    다이내믹 프로그래밍2: 가치 이터레이션 - 명시적인 정책/내재적인 정책   
                                         벨만 최적방정식과 가치 이터레이션   
    다이내믹 프로그래밍의 한계 - 계산 복잡도, 차원의 저주, 환경에 대한 완벽한 정보 필요
    강화학습 - 모델 없이 환경과의 상호작용을 통해 입력과 출력 사이의 관계를 학습   
    - 4장 강화학습 기초3:그리드월드와 큐러닝   
    강화학습과 정책 평가 1: 몬테카를로 예측 - 기댓값을 샘플링을 통한 평균으로 대체하는 기법   
                                           에피소드 하나를 진행하고 에피소드 동안 지나온 상태의 반환값을 구함
                                           반환값은 하나의 샘플이 되어 각 상태의 가치함수를 업데이트   
    강화학습과 정책 평가 2:시간차 예측 - 몬테카를로와 달리 타임스텝마다 큐함수를 업데이트 / 벨만 기대 방정식 이용   
    강화학습 알고리즘 1: 살사 - 강화학습 제어에서 행동 선택 시 가치함수룰 사용하면 환경의 모델을 알아야 하기 때문에 큐함수를 사용   
                              시간차 제어에서는 하나의 샘플로 (s,a,r,s',a')가 필요. 시간차제어를 살사라고 함   
    강화학습 알고리즘 2: 큐러닝 - 살사:온폴리시 / 큐러닝:오프폴리시(온폴리시의 단점을 개선)
                                큐함수 업데이트에 벨만 최적방정식 이용   
    - 5장 강화학습 심화 1: 그리드월드와 근사함수   
    근사함수 - 몬테카를로, 살사, 큐러닝 -> 테이블 방식의 한계 -> 근사함수를 통한 가치함수의 매개변수화를 해야함   
    인공신경망 - 노드, 활성함수   
                딥러닝 : 높은 추상화 - 심층 신경망이 스스로 특징 추출   
    신경망의 학습 - 오차함수(Lose Function), 역전파 알고리즘(편미분을 이용해 가중치, 편향 업데이트), 경사 하강법(변수: 학습 속도)   